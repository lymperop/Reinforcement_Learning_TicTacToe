{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b005bd1d",
   "metadata": {},
   "source": [
    "# 1. Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa9eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dd1ea",
   "metadata": {},
   "source": [
    "# 2. Define players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7e469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "\n",
    "    @staticmethod\n",
    "    def show_board(board):\n",
    "        # Show the board\n",
    "        print('|'.join(board[6:9]))\n",
    "        print('|'.join(board[3:6]))\n",
    "        print('|'.join(board[0:3]))\n",
    "\n",
    "\n",
    "# Create human player\n",
    "class HumanPlayer(Player):\n",
    "    # Inherit Player class\n",
    "\n",
    "    def reward(self, value, board):\n",
    "        # Human player does not care about reward\n",
    "        pass\n",
    "\n",
    "    def make_move(self, board):\n",
    "        # Human has to put a valid input; Try until the input is valid\n",
    "        while True:\n",
    "            try:\n",
    "                self.show_board(board)            \n",
    "                clear_output(wait=True)\n",
    "                move = input('Your next move (cell index 1-9):')\n",
    "                move = int(move)\n",
    "                if not (move - 1 in range(9)) or board[move-1]!=' ' :\n",
    "                    raise ValueError\n",
    "            except ValueError:\n",
    "                print('Invalid move; try again:\\n')\n",
    "            else:\n",
    "                return move - 1\n",
    "\n",
    "\n",
    "# Create AI player\n",
    "class AIPlayer(Player):\n",
    "\n",
    "    def __init__(self, epsilon=0.4, alpha=0.3, gamma=0.9, default_q=1):\n",
    "        # epsilon: probability of exploration\n",
    "        self.EPSILON = epsilon\n",
    "        # learning rate\n",
    "        self.ALPHA = alpha\n",
    "        # discount parameter\n",
    "        self.GAMMA = gamma\n",
    "        # given state is not defined yet\n",
    "        self.DEFAULT_Q = default_q\n",
    "        # dictionary of states Q(s,a): return a reward for a state-action pair\n",
    "        self.q = {}\n",
    "        # previous move\n",
    "        self.move = None\n",
    "        # board in previous iteration\n",
    "        self.board = (' ',) * 9\n",
    "\n",
    "    def available_moves(self, board):\n",
    "        # empty cells on the board so far\n",
    "        return [i for i in range(9) if board[i] == ' ']\n",
    "\n",
    "    def get_q(self, state, action):\n",
    "        # return Q value, and create a new one if it is not in the dictionary yet\n",
    "        if self.q.get((state, action)) is None:\n",
    "            self.q[(state, action)] = self.DEFAULT_Q\n",
    "        return self.q[(state, action)]\n",
    "\n",
    "    def make_move(self, board):\n",
    "        # make a random move with epsilon probability or pick the action with the highest Q value\n",
    "        self.board = tuple(board)\n",
    "        # retrieve available moves\n",
    "        actions = self.available_moves(board)\n",
    "\n",
    "        if random.random() < self.EPSILON:\n",
    "            # exploration\n",
    "            self.move = random.choice(actions)\n",
    "        else:\n",
    "            # exploitation\n",
    "            q_values = [self.get_q(self.board, a) for a in actions]\n",
    "            max_q_value = max(q_values)\n",
    "            # if there are multiple best actions, choose one at random\n",
    "            if q_values.count(max_q_value) > 1:\n",
    "                best_actions = [i for i in range(len(actions)) if q_values[i] == max_q_value]\n",
    "                best_move = actions[random.choice(best_actions)]\n",
    "            else:\n",
    "                best_move = actions[q_values.index(max_q_value)]\n",
    "            self.move = best_move\n",
    "        return self.move\n",
    "\n",
    "    def reward(self, reward, board):\n",
    "        # update Q(s,a)\n",
    "        if self.move:\n",
    "            prev_q = self.get_q(self.board, self.move)\n",
    "            max_q_new = max([self.get_q(tuple(board), a) for a in self.available_moves(self.board)])\n",
    "            self.q[(self.board, self.move)] = prev_q + self.ALPHA * (reward + self.GAMMA * max_q_new - prev_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0a98bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    BLANK = ' '\n",
    "    AI_PLAYER = 'X'\n",
    "    HUMAN_PLAYER = '0'\n",
    "    REWARD_WIN = 10\n",
    "    REWARD_LOSE = -10\n",
    "    REWARD_TIE = -1\n",
    "\n",
    "    # implement game logic\n",
    "    def __init__(self, player1, player2):\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.isHuman = isinstance(player1, HumanPlayer) or isinstance(player2, HumanPlayer)\n",
    "        # randomly choose which player starts first\n",
    "        self.first_player_turn = random.choice([True, False])\n",
    "        self.board = [' '] * 9\n",
    "\n",
    "    def play(self):\n",
    "        # this is the game loop\n",
    "        while True:\n",
    "            if self.first_player_turn:\n",
    "                player = self.player1\n",
    "                other_player = self.player2\n",
    "                player_tickers = (self.AI_PLAYER, self.HUMAN_PLAYER)\n",
    "            else:\n",
    "                player = self.player2\n",
    "                other_player = self.player1\n",
    "                player_tickers = (self.HUMAN_PLAYER, self.AI_PLAYER)\n",
    "\n",
    "            # check game (win, lose, draw)\n",
    "            game_over, winner = self.is_game_over(player_tickers)\n",
    "            # game over: handle rewards\n",
    "            if game_over:\n",
    "                if winner == player_tickers[0]:\n",
    "                    player.reward(self.REWARD_WIN, self.board[:])\n",
    "                    other_player.reward(self.REWARD_LOSE, self.board[:])\n",
    "                    if self.isHuman:\n",
    "                        other_player.show_board(self.board)\n",
    "                        print('\\n %s won!' % player.__class__.__name__)\n",
    "                elif winner == player_tickers[1]:\n",
    "                    player.reward(self.REWARD_LOSE, self.board[:])\n",
    "                    other_player.reward(self.REWARD_WIN, self.board[:])\n",
    "                    if self.isHuman:\n",
    "                        other_player.show_board(self.board)\n",
    "                        print('\\n %s won!' % other_player.__class__.__name__)\n",
    "                else:\n",
    "                    player.reward(self.REWARD_TIE, self.board[:])\n",
    "                    other_player.reward(self.REWARD_TIE, self.board[:])\n",
    "                    if self.isHuman:\n",
    "                        other_player.show_board(self.board)\n",
    "                        print('Tie!')\n",
    "                break\n",
    "            # next player's turn   \n",
    "            self.first_player_turn = not self.first_player_turn\n",
    "            # actual player's best move ( based on Q(s, a) table of AI player)\n",
    "            move = player.make_move(self.board)        \n",
    "            self.board[move] = player_tickers[0]\n",
    "\n",
    "    def is_game_over(self, player_tickers):\n",
    "        # consider both players\n",
    "        for player_ticker in player_tickers:\n",
    "\n",
    "            # check horizontal dimension\n",
    "            for i in range(3):\n",
    "                if self.board[3 * i + 0] == player_ticker and \\\n",
    "                        self.board[3 * i + 1] == player_ticker and \\\n",
    "                        self.board[3 * i + 2] == player_ticker:\n",
    "                    return True, player_ticker\n",
    "            # check vertical dimension\n",
    "            for j in range(3):\n",
    "                if self.board[j + 0] == player_ticker and \\\n",
    "                        self.board[j + 3] == player_ticker and \\\n",
    "                        self.board[j + 6] == player_ticker:\n",
    "                    return True, player_ticker\n",
    "            # check diagonals\n",
    "            if self.board[0] == player_ticker and self.board[4] == player_ticker and self.board[8] == player_ticker:\n",
    "                return True, player_ticker\n",
    "            if self.board[2] == player_ticker and self.board[4] == player_ticker and self.board[6] == player_ticker:\n",
    "                return True, player_ticker\n",
    "\n",
    "        # return draw\n",
    "        if self.board.count(' ') == 0:\n",
    "            return True, None\n",
    "        else:\n",
    "            return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0291087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the AI players...\n",
      "Training is Done\n"
     ]
    }
   ],
   "source": [
    "TRAINING_EPOCHS = 100000\n",
    "TRAINING_EPSILON = 0.2\n",
    "\n",
    "# train AI player\n",
    "ai_player_1 = AIPlayer()\n",
    "ai_player_2 = AIPlayer()\n",
    "print('Training the AI players...')\n",
    "ai_player_1.EPSILON = TRAINING_EPSILON\n",
    "ai_player_2.EPSILON = TRAINING_EPSILON\n",
    "\n",
    "# training\n",
    "for _ in range(TRAINING_EPOCHS):\n",
    "    game = TicTacToe(ai_player_1, ai_player_2)\n",
    "    game.play()\n",
    "\n",
    "print('Training is Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d9d34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your next move (cell index 1-9):6\n",
      "X|0|X\n",
      "X|0|0\n",
      "0|X|X\n",
      "Tie!\n"
     ]
    }
   ],
   "source": [
    "# After training, set exploration to 0, the AI player will always play optimally\n",
    "ai_player_1.EPSILON = 0\n",
    "human_player = HumanPlayer()\n",
    "game = TicTacToe(ai_player_1, human_player)\n",
    "game.play()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba4fe1",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
